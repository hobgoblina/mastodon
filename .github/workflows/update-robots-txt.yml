name: Update robots.txt

on: 
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:

env:
  ROBOTS_PATH: public/robots.txt
  
jobs:
  update-robots:
    runs-on: ubuntu-latest
    steps: 
      - name: Checkout
        uses: actions/checkout@v3
      - name: Update robot.txt
        env:
          API_KEY: ${{ secrets.ROBOTS }}
        run: |
          cp robots-base.txt $ROBOTS_PATH
          curl --location 'https://api.darkvisitors.com/robots-txts' \
          --header 'Content-Type: application/json' \
          --header "Authorization: Bearer $API_KEY" \
          --data '{ "agent_types": [ "AI Data Scraper", "AI Assistant", "AI Search Crawler" ], "disallow": "/" }' >> $ROBOTS_PATH
      - name: Create pull request
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: robots.txt-update
          title: Update robots.txt
          commit-message: Update robots.txt
          labels: robots.txt
          add-paths: ${{ env.ROBOTS_PATH }}
          reviewers: hobgoblina
          body: This PR was generated by the `Update robots.txt` action and contains updates to our robots.txt file, pulled from [Dark Visitors](https://darkvisitors.com/).
