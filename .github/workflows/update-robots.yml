name: Update robots.txt

permissions: write-all

on: 
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:

jobs:
  update-robots:
    runs-on: ubuntu-latest
    steps: 
      - name: Create Branch
        env:
          GITHUB_TOKEN: ${{ github.token }}
        uses: peterjgrainger/action-create-branch@v2.2.0
        with:
          branch: robots.txt-update
      - name: Checkout
        uses: actions/checkout@v3
        with:
          ref: robots.txt-update
          fetch-depth: 0 
      - name: Update robot.txt
        id: update
        env:
          API_KEY: ${{ secrets.ROBOTS }}
        run: |
          cp robots-base.txt public/robots.txt
          curl --location 'https://api.darkvisitors.com/robots-txts' \
          --header 'Content-Type: application/json' \
          --header 'Authorization: Bearer $API_KEY' \
          --data '{ "agent_types": [ "AI Data Scraper", "AI Assistant", "AI Search Crawler" ], "disallow": "/" }' >> public/robots.txt
          git add public/robots.txt
          git commit -m "update robots.txt"
          changes=$(git push origin 2>&1)
          if [ "$changes" = "Everything up-to-date" ]; then
            echo "skip=true" >> "$GITHUB_OUTPUT"
          fi
      - name: Create Pull Request
        if: |
          !steps.update.outputs.skip
        uses: actions/github-script@v6
        with:
          script: |
            const { repo, owner } = context.repo;
            const result = await github.rest.pulls.create({
              title: 'Update robots.txt',
              owner,
              repo,
              head: 'robots.txt-update',
              base: 'dev',
              body: 'This PR was *auto-generated* by the `Update robots.txt` action and contains updates to our robots.txt file, pulled from [Dark Visitors](https://darkvisitors.com/).'
            });
            github.rest.issues.addLabels({
              owner,
              repo,
              issue_number: result.data.number,
              labels: ['automated pr']
            });
